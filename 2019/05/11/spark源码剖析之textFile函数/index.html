<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="努力成为一名实力很强的小白"><title>spark源码剖析之textFile函数 | TiuVe</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="stylesheet" type="text/css" href="/css/donate.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">spark源码剖析之textFile函数</h1><a id="logo" href="/.">TiuVe</a><p class="description">气蒸云梦泽，波撼岳阳城</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/./archives/"><i class="fa fa-archive"> 归档</i></a><a href="/./about/about.html"><i class="fa fa-user"> 关于</i></a><a href="/./project/project.html"><i class="fa fa-archive"> work</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">spark源码剖析之textFile函数</h1><div class="post-meta"><a href="/2019/05/11/spark源码剖析之textFile函数/#comments" class="comment-count"></a><p><span class="date">May 11, 2019</span></p></div><div class="post-content"><p>最近因为工作需要，打算阅读一下spark的源码。首先研究一下spark是如何读入数据的。关于spark读取数据的机制，其实知乎上已经有大牛回答了，见@连城（<a href="https://www.zhihu.com/question/23079001" target="_blank" rel="noopener">内存有限的情况下 Spark 如何处理 T 级别的数据？</a>），如果对spark源码已经有一定的了解，该回答还是非常形象易懂的。</p>
<h2 id="一-阅读spark源码的姿势"><a href="#一-阅读spark源码的姿势" class="headerlink" title="一 阅读spark源码的姿势"></a>一 阅读spark源码的姿势</h2><p>因为我并非系统地研究spark,只是想在需要的时候查询一下某个函数的实现，所以可以直接在IDEA中，选中某个函数，使用快捷键ctrl+b（mac下是cmd+b），查看该函数的实现，这时候会提示 download source code，按照提示下载之后，再使用该命令就可以查看函数实现。</p>
<p><img src="/images/idea_source_code.png"></p>
<h2 id="二-spark数据读取机制"><a href="#二-spark数据读取机制" class="headerlink" title="二 spark数据读取机制"></a>二 spark数据读取机制</h2><h3 id="2-1-textFile函数"><a href="#2-1-textFile函数" class="headerlink" title="2.1 textFile函数"></a>2.1 textFile函数</h3><p>textFile函数是spark中的数据读取函数，其path参数可以是HDFS，本地文件，或者其它hadoop支持的文件系统地URL，其返回类型是 RDD[String]。</p>
<p>minPartitions&#x3D;  math.min(defaultParallelism, 2) 是指定数据的分区，如果不指定分区，当你的核数大于2的时候，不指定分区数那么就是 2</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Read a text file from HDFS, a local file system (available on all nodes), or any</span></span><br><span class="line"><span class="comment"> * Hadoop-supported file system URI, and return it as an RDD of Strings.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>textFile函数读取数据的n种姿势：</p>
<ol>
<li>读取当前目录下的一个文件</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"hello.txt"</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(path)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>读取当前目录下的多个文件</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"hello1.txt,hello2.txt"</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(path)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>从本地文件系统读取一个文件</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"file:///usr/local/spark/hello.txt"</span>  <span class="comment">//local file</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>读取hdfs的一个目录</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"hdfs://xxx/xxx/traindata/"</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(path)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>通配符</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"hdfs://xxx/xxx/traindata/2019051120*"</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(path)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-RDD-的转换"><a href="#2-2-RDD-的转换" class="headerlink" title="2.2 RDD 的转换"></a>2.2 RDD 的转换</h3><p>之所以要讲spark rdd的转换和计算流程(章节2.3)，是因为spark数据读取机制是基于spark rdd转换和计算的。众所周知，spark是惰性计算的，在spark中有两种操作：transform 和 action。spark会将所有的transform操作连接成图，如果遇到action操作就计算该图。也就是说，使用textFile函数并非立即读取数据的，而是等到执行action操作的时候才会真正地读取数据。</p>
<p>首先给出结论：在spark内部，单个executor进程内rdd的分片数据是流式访问的。以下面代码为例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rdd = sc.textFile(<span class="string">"hdfs://xxx/hello.txt"</span>);  </span><br><span class="line"><span class="keyword">var</span> rdd_new = rdd.map(_.split(<span class="string">","</span>));    </span><br><span class="line">print(rdd_new.count())；</span><br></pre></td></tr></table></figure>

<p>上面代码比较简单，不再介绍。我们再看一下textFile函数的实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] </span><br><span class="line">= withScope &#123;</span><br><span class="line">	assertNotStopped()</span><br><span class="line">	hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，textFile函数的核心是，<strong>调用了一个hadoopFile函数，然后进行了一个map操作，获取了第二个元素</strong>。hadoopFile的参数除了path之外，还有 classOf[TextInputFormat], classOf[LongWritable], classOf[Text]。如果有过hadoop的开发经验，肯定对这三个参数比较熟悉。最后还有一个minPartitions参数。</p>
<p>然后我们看一下hadoopFile对象是什么样子的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Get an RDD for a Hadoop file with an arbitrary InputFormat</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each</span></span><br><span class="line"><span class="comment"> * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle</span></span><br><span class="line"><span class="comment"> * operation will create many references to the same object.</span></span><br><span class="line"><span class="comment"> * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first</span></span><br><span class="line"><span class="comment"> * copy them using a `map` function.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hadoopFile</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    inputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">InputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]],</span><br><span class="line">    keyClass: <span class="type">Class</span>[<span class="type">K</span>],</span><br><span class="line">    valueClass: <span class="type">Class</span>[<span class="type">V</span>],</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="comment">// A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.</span></span><br><span class="line">  <span class="keyword">val</span> confBroadcast = broadcast(<span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(hadoopConfiguration))</span><br><span class="line">  <span class="keyword">val</span> setInputPathsFunc = (jobConf: <span class="type">JobConf</span>) =&gt; <span class="type">FileInputFormat</span>.setInputPaths(jobConf, path)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">HadoopRDD</span>(</span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    confBroadcast,</span><br><span class="line">    <span class="type">Some</span>(setInputPathsFunc),</span><br><span class="line">    inputFormatClass,</span><br><span class="line">    keyClass,</span><br><span class="line">    valueClass,</span><br><span class="line">    minPartitions).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，hadoopFile返回了k-v格式的rdd。在hadoopFile内部，首先对hadoopConfiguration进行了广播，然后设置了一个setInputPathsFunc 函数。最重要的，在hadoopFile函数内部，新建了一个HadoopRDD对象并返回。也就是说，textFile函数中实际上是调用了HadoopRDD对象的map函数。而通过查看HadoopRDD的源码可以发现，class HadoopRDD是继承自抽象类RDD的。此外，在类HadoopRDD中，并没有重写类RDD中的map，reduce函数，也就是说，所有继承了RDD类的对象都是直接调用抽象类RDD中的map，reduce等方法。</p>
<p>在抽象类中的map方法长这个样子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private var _sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private var deps: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]</span></span></span><br><span class="line"><span class="class"><span class="params">  </span>) <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new RDD by applying a function to all elements of this RDD.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="comment">// clean方法实际上调用了ClosureCleaner的clean方法，旨在清除闭包中的不能序列化的变量，防止RDD在网络传输过程中反序列化失败[1]</span></span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，map方法首先对穿传进来的函数进行clean操作，然后构造了一个MapPartitionsRDD对象，它的参数我们一会再分析。这里可以确定一点，即到目前为止并没有进行数据读取，计算操作。</p>
<p>再看第二行代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rdd_new = rdd.map(_.split(<span class="string">","</span>));</span><br></pre></td></tr></table></figure>

<p>通过上面的分析可知，这里的rdd其实是一个MapPartitionsRDD对象，同样地，在执行了map操作之后，也返回了一个MapPartitionsRDD对象。</p>
<p>再看最后一行代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(rdd_new.count())；</span><br></pre></td></tr></table></figure>

<p>这里的count是一个action操作。前面提到，action操作会触发spark的图计算操作。我们可以看一下count的实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return the number of elements in the RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>

<p>可以看到，在count内部执行了sc.runJob()操作。runJob函数会触发DagScheduler去分解任务并提交到集群执行。</p>
<h3 id="2-3-RDD的计算"><a href="#2-3-RDD的计算" class="headerlink" title="2.3 RDD的计算"></a>2.3 RDD的计算</h3><p>在action操作触发了spark的图计算之后，该任务将会被分解执行。具体地，task首先会执行最后一个rdd的compute方法。在上述代码中，最后一个rdd是一个MapPartitionsRDD对象。我们看一下MapPartitionsRDD的compute函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An RDD that applies the provided function to every partition of the parent RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsRDD</span>[<span class="type">U</span>: <span class="type">ClassTag</span>, <span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    var prev: <span class="type">RDD</span>[<span class="type">T</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    f: (<span class="type">TaskContext</span>, <span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]</span>) <span class="title">=&gt;</span> <span class="title">Iterator</span>[<span class="type">U</span>],  <span class="title">//</span> (<span class="params"><span class="type">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class="line"><span class="class">    <span class="title">preservesPartitioning</span></span>: <span class="type">Boolean</span> = <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">U</span>](prev) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> partitioner = <span class="keyword">if</span> (preservesPartitioning) firstParent[<span class="type">T</span>].partitioner <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = firstParent[<span class="type">T</span>].partitions</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">U</span>] =</span><br><span class="line">    f(context, split.index, firstParent[<span class="type">T</span>].iterator(split, context))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">clearDependencies</span></span>() &#123;</span><br><span class="line">    <span class="keyword">super</span>.clearDependencies()</span><br><span class="line">    prev = <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，compute函数的参数有两个：分区 split 和 Task上下文，在compute函数内部实际上调用了 f 函数。f函数是怎么来的呢？它其实是类MapPartitionsRDD的构造函数的参数，在构造MapPartitionsRDD对象的时候被传递进来的。上面分析提到，MapPartitionsRDD对象是在抽象类RDD的map函数内部构造的，f 函数也是在该地方实现的。f函数的参数有三个：第一个是task上下文，第二个是分区索引，第三个是父rdd的迭代器。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f);</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，f 函数的实现就是 (this, (context, pid, iter) &#x3D;&gt; iter.map(cleanF) 部分，f函数实际上调用了第三个函数参数的map方法，map方法的参数是用户调用map函数时传入的函数，本例中是split()。刚刚已经提到，f函数的第三个参数是父rdd(因为该MapPartitionsRDD的父rdd也是MapPartitionsRDD，而MapPartitionsRDD中没有iterator函数的实现，所以MapPartitionsRDD实际上调用了抽象类RDD中的)的迭代器方法iterator，我们可以看一下它是什么样子。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Internal method to this RDD; will read from cache if applicable, or otherwise compute it.</span></span><br><span class="line"><span class="comment"> * This should ''not'' be called by users directly, but is available for implementors of custom</span></span><br><span class="line"><span class="comment"> * subclasses of RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">    <span class="type">SparkEnv</span>.get.cacheManager.getOrCompute(<span class="keyword">this</span>, split, context, storageLevel)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    computeOrReadCheckpoint(split, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，iterator函数首先判断，该rdd的storageLevel是否为NONE，如果不为NONE，则尝试从缓存中读取数据，如果缓存中没有，则通过计算获取对应分区数据的迭代器。如果该rdd的storageLevel为NONE，则尝试从checkpoint获取对应分区数据的迭代器，如果checkpoint不存在则通过计算获取。</p>
<p>iterator会返回一个迭代器，可以通过该迭代器访问父rdd的某个分区中的每个元素。如果内存在没有父rdd的数据，则调用父rdd的compute方法进行计算。</p>
<p>我们可以再看一下computeOrReadCheckpoint这个方法 ( 这个方法比较简单，好分析 ) 。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">computeOrReadCheckpoint</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] =</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span> (isCheckpointedAndMaterialized) &#123;</span><br><span class="line">    firstParent[<span class="type">T</span>].iterator(split, context)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    compute(split, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，在computeOrReadCheckpoint内部，通过调用父rdd的compute方法获取父rdd的split分区的迭代器。</p>
<p>可以想象，经过一层层transform操作溯源之后，最终会调用类HadoopRDD中的compute函数，它长这个样子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(theSplit: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">InterruptibleIterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> iter = <span class="keyword">new</span> <span class="type">NextIterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> split = theSplit.asInstanceOf[<span class="type">HadoopPartition</span>]</span><br><span class="line">    logInfo(<span class="string">"Input split: "</span> + split.inputSplit)</span><br><span class="line">    <span class="keyword">val</span> jobConf = getJobConf()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> inputMetrics = context.taskMetrics.getInputMetricsForReadMethod(<span class="type">DataReadMethod</span>.<span class="type">Hadoop</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Sets the thread local variable for the file's name</span></span><br><span class="line">    split.inputSplit.value <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> fs: <span class="type">FileSplit</span> =&gt; <span class="type">SqlNewHadoopRDDState</span>.setInputFileName(fs.getPath.toString)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="type">SqlNewHadoopRDDState</span>.unsetInputFileName()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Find a function that will return the FileSystem bytes read by this thread. Do this before</span></span><br><span class="line">    <span class="comment">// creating RecordReader, because RecordReader's constructor might read some bytes</span></span><br><span class="line">    <span class="keyword">val</span> bytesReadCallback = inputMetrics.bytesReadCallback.orElse &#123;</span><br><span class="line">      split.inputSplit.value <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">FileSplit</span> | _: <span class="type">CombineFileSplit</span> =&gt;</span><br><span class="line">          <span class="type">SparkHadoopUtil</span>.get.getFSBytesReadOnThreadCallback()</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    inputMetrics.setBytesReadCallback(bytesReadCallback)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> reader: <span class="type">RecordReader</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> inputFormat = getInputFormat(jobConf)</span><br><span class="line">    <span class="type">HadoopRDD</span>.addLocalConfiguration(<span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMMddHHmm"</span>).format(createTime),</span><br><span class="line">      context.stageId, theSplit.index, context.attemptNumber, jobConf)</span><br><span class="line">    reader = inputFormat.getRecordReader(split.inputSplit.value, jobConf, <span class="type">Reporter</span>.<span class="type">NULL</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Register an on-task-completion callback to close the input stream.</span></span><br><span class="line">    context.addTaskCompletionListener&#123; context =&gt; closeIfNeeded() &#125;</span><br><span class="line">    <span class="keyword">val</span> key: <span class="type">K</span> = reader.createKey()</span><br><span class="line">    <span class="keyword">val</span> value: <span class="type">V</span> = reader.createValue()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getNext</span></span>(): (<span class="type">K</span>, <span class="type">V</span>) = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        finished = !reader.next(key, value)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> eof: <span class="type">EOFException</span> =&gt;</span><br><span class="line">          finished = <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!finished) &#123;</span><br><span class="line">        inputMetrics.incRecordsRead(<span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      (key, value)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>() &#123;</span><br><span class="line">      <span class="keyword">if</span> (reader != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="type">SqlNewHadoopRDDState</span>.unsetInputFileName()</span><br><span class="line">        <span class="comment">// Close the reader and release it. Note: it's very important that we don't close the</span></span><br><span class="line">        <span class="comment">// reader more than once, since that exposes us to MAPREDUCE-5918 when running against</span></span><br><span class="line">        <span class="comment">// Hadoop 1.x and older Hadoop 2.x releases. That bug can lead to non-deterministic</span></span><br><span class="line">        <span class="comment">// corruption issues when reading compressed input.</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          reader.close()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">            <span class="keyword">if</span> (!<span class="type">ShutdownHookManager</span>.inShutdown()) &#123;</span><br><span class="line">              logWarning(<span class="string">"Exception in RecordReader.close()"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          reader = <span class="literal">null</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (bytesReadCallback.isDefined) &#123;</span><br><span class="line">          inputMetrics.updateBytesRead()</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (split.inputSplit.value.isInstanceOf[<span class="type">FileSplit</span>] ||</span><br><span class="line">                   split.inputSplit.value.isInstanceOf[<span class="type">CombineFileSplit</span>]) &#123;</span><br><span class="line">          <span class="comment">// If we can't get the bytes read from the FS stats, fall back to the split size,</span></span><br><span class="line">          <span class="comment">// which may be inaccurate.</span></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            inputMetrics.incBytesRead(split.inputSplit.value.getLength)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: java.io.<span class="type">IOException</span> =&gt;</span><br><span class="line">              logWarning(<span class="string">"Unable to get input size to set InputMetrics for task"</span>, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[(<span class="type">K</span>, <span class="type">V</span>)](context, iter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="三-Spark在内存有限情况下的执行机制"><a href="#三-Spark在内存有限情况下的执行机制" class="headerlink" title="三 Spark在内存有限情况下的执行机制"></a>三 Spark在内存有限情况下的执行机制</h2><p>正如知乎<a href="https://www.zhihu.com/question/23079001" target="_blank" rel="noopener">连城</a>所说，spark内部使用迭代器流式访问数据，用这个Iterator访问整个数据集，空间复杂度是O(1)。可见，Spark RDD的immutable语义并不会造成大数据内存计算任务的庞大内存开销。然而，如果spark任务被划分为多个stage，那么在 stage 0 中，通过流式访问的机制，如果内存足够大，可以容纳经过各种transform操作之后的数据，那么 stage 0 并不需要考虑内存的问题，但是如果 stage 0 和 stage 1 是通过reduce操作划分的，那么就会涉及shuffle。</p>
<p>在 shuffle 过程中，前一个 stage 的 ShuffleMapTask 进行 shuffle write， 把数据存储在 blockManager 上面，并且把数据位置元信息上报到 driver 的 mapOutTrack 组件中，下一个 stage 根据数据位置元信息，进行 shuffle read， 拉取上个 stage 的输出数据。例如：在原始数据中有10个字段（大小100g），在 stage 0 中经过各种transform后只留下4个字段（大小40g），那么stage 0 的ShuffleMapTask 进行 shuffle write，将40g的数据写到blockManager中，供 stage 1 进行shuffle读。</p>
<p>引用 知乎<a href="https://www.zhihu.com/question/23079001" target="_blank" rel="noopener">连城</a>的回答：</p>
<blockquote>
<p>在Spark内部，单个executor进程内RDD的分片数据是用Iterator流式访问的，Iterator的hasNext方法和next方法是由RDD lineage上各个transformation携带的闭包函数复合而成的。该复合Iterator每访问一个元素，就对该元素应用相应的复合函数，得到的结果再流式地落地（对于shuffle stage是落地到本地文件系统留待后续stage访问，对于result stage是落地到HDFS或送回driver端等等，视选用的action而定）。如果用户没有要求Spark cache该RDD的结果，那么这个过程占用的内存是很小的，一个元素处理完毕后就落地或扔掉了（概念上如此，实现上有buffer），并不会长久地占用内存。只有在用户要求Spark cache该RDD，且storage level要求在内存中cache时，Iterator计算出的结果才会被保留，通过cache manager放入内存池。</p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="https://blog.csdn.net/legotime/article/details/51871724" target="_blank" rel="noopener">Spark从外部读取数据之textFile</a></li>
<li><a href="https://blog.csdn.net/u013761049/article/details/82492581" target="_blank" rel="noopener">spark中ClosureClean中的clean方法</a></li>
<li><a href="https://www.cnblogs.com/candl/p/9604351.html" target="_blank" rel="noopener">Spark RDD深度解析-RDD计算流程</a></li>
<li><a href="https://www.zhihu.com/question/23079001" target="_blank" rel="noopener">内存有限的情况下 Spark 如何处理 T 级别的数据？</a></li>
<li><a href="https://toutiao.io/posts/eicdjo/preview" target="_blank" rel="noopener">彻底搞懂 Spark 的 shuffle 过程（shuffle write）</a></li>
</ol>
</div><div class="tags"><a href="/tags/spark/">spark</a><a href="/tags/textFile/">textFile</a><a href="/tags/源码剖析/">源码剖析</a><a href="/tags/rdd计算/">rdd计算</a></div><script type="text/javascript" src="/js/jquery.js?v=2.0.1" async></script><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" href="javascript:;" title="打赏" class="btn_donate"></a><div class="donate_txt"> &uarr;<br>谢谢~ 您的支持将鼓励我继续创作！<br></div></div><div id="donate_guide" class="donate_bar center hidden pay"><img src="/img/weChatMoney.png" title="微信打赏" alt="微信打赏"><img src="/img/alipayMoney.png" title="支付宝打赏" alt="支付宝打赏"></div><script type="text/javascript">document.getElementById('btn_donate').onclick = function(){
    $('#donate_board').addClass('hidden');
    $('#donate_guide').removeClass('hidden');
}</script></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2019/05/12/mac下配置多个git账户/" class="pre">mac下配置多个git账户</a><a href="/2018/12/17/trie树/" class="next">trie树</a></div><div id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80MTIyOC8xNzc3Ng=="></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一-阅读spark源码的姿势"><span class="toc-text">一 阅读spark源码的姿势</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二-spark数据读取机制"><span class="toc-text">二 spark数据读取机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-textFile函数"><span class="toc-text">2.1 textFile函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-RDD-的转换"><span class="toc-text">2.2 RDD 的转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-RDD的计算"><span class="toc-text">2.3 RDD的计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三-Spark在内存有限情况下的执行机制"><span class="toc-text">三 Spark在内存有限情况下的执行机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/08/10/《Parameter-Efficient-Transfer-from-Sequential-Behaviors-for-User-Modeling-and-Recommendation》论文阅读/">《Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation》论文阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/07/NLP相关资料整理/">NLP相关资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/hexo博客链接在微信被屏蔽的解决办法/">hexo博客链接在微信被屏蔽的解决办法</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/TensorFlow的自动求导具体是在哪部分代码里实现的？/">TensorFlow的自动求导具体是在哪部分代码里实现的？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/tf中如何修改tensor的值/">tf中如何修改tensor的值</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/04/解决tf1-15中tf-scatter-update-函数没有定义梯度的问题/">解决tf1.15中tf.scatter_update()函数没有定义梯度的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/31/解决hexo博客代码在IOS下字体过大的问题/">解决hexo博客代码在IOS下字体过大的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/30/记一次悲催的tf报错/">记一次悲催的tf报错</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/17/tensorboard中的Smoothing/">tensorboard中的Smoothing</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/11/25/如何打包maven项目中的配置文件和依赖/">如何打包maven项目中的配置文件和依赖</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DMMLAI/">DMMLAI</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo主题/">Hexo主题</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hive/">Hive</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PL/">PL</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hole/">hole</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/mac/">mac</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sklearn/">sklearn</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/例行化/">例行化</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发/">开发</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发工具/">开发工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐/">推荐</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/摄影/">摄影</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据获取/">数据获取</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/游记/">游记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学/">统计学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计划/">计划</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算广告/">计算广告</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发/">高并发</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/C-C/" style="font-size: 15px;">C/C++</a> <a href="/tags/cold/" style="font-size: 15px;">cold</a> <a href="/tags/gwen/" style="font-size: 15px;">gwen</a> <a href="/tags/FM/" style="font-size: 15px;">FM</a> <a href="/tags/FTRL/" style="font-size: 15px;">FTRL</a> <a href="/tags/Java编程思想/" style="font-size: 15px;">Java编程思想</a> <a href="/tags/L2范数，范数/" style="font-size: 15px;">L2范数，范数</a> <a href="/tags/hive/" style="font-size: 15px;">hive</a> <a href="/tags/linux性能管理/" style="font-size: 15px;">linux性能管理</a> <a href="/tags/kmp/" style="font-size: 15px;">kmp</a> <a href="/tags/字符串匹配/" style="font-size: 15px;">字符串匹配</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/sublime/" style="font-size: 15px;">sublime</a> <a href="/tags/TFRecord/" style="font-size: 15px;">TFRecord</a> <a href="/tags/指标/" style="font-size: 15px;">指标</a> <a href="/tags/auc/" style="font-size: 15px;">auc</a> <a href="/tags/cpm/" style="font-size: 15px;">cpm</a> <a href="/tags/opcm/" style="font-size: 15px;">opcm</a> <a href="/tags/ecpm/" style="font-size: 15px;">ecpm</a> <a href="/tags/zookepper/" style="font-size: 15px;">zookepper</a> <a href="/tags/centos7/" style="font-size: 15px;">centos7</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/hexo，微信屏蔽/" style="font-size: 15px;">hexo，微信屏蔽</a> <a href="/tags/tf/" style="font-size: 15px;">tf</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/自动求导，自动微分/" style="font-size: 15px;">自动求导，自动微分</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/重装系统/" style="font-size: 15px;">重装系统</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/dfs/" style="font-size: 15px;">dfs</a> <a href="/tags/递归/" style="font-size: 15px;">递归</a> <a href="/tags/poj/" style="font-size: 15px;">poj</a> <a href="/tags/OJ/" style="font-size: 15px;">OJ</a> <a href="/tags/暴力/" style="font-size: 15px;">暴力</a> <a href="/tags/棋盘规则/" style="font-size: 15px;">棋盘规则</a> <a href="/tags/dp/" style="font-size: 15px;">dp</a> <a href="/tags/质数/" style="font-size: 15px;">质数</a> <a href="/tags/素数/" style="font-size: 15px;">素数</a> <a href="/tags/回文字符串/" style="font-size: 15px;">回文字符串</a> <a href="/tags/状态压缩/" style="font-size: 15px;">状态压缩</a> <a href="/tags/BFS/" style="font-size: 15px;">BFS</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/pytorch/" style="font-size: 15px;">pytorch</a> <a href="/tags/module/" style="font-size: 15px;">module</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/shell/" style="font-size: 15px;">shell</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/roc-auc-score/" style="font-size: 15px;">roc_auc_score</a> <a href="/tags/hole/" style="font-size: 15px;">hole</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/打散/" style="font-size: 15px;">打散</a> <a href="/tags/svm多类别分类/" style="font-size: 15px;">svm多类别分类</a> <a href="/tags/网格搜索/" style="font-size: 15px;">网格搜索</a> <a href="/tags/textFile/" style="font-size: 15px;">textFile</a> <a href="/tags/源码剖析/" style="font-size: 15px;">源码剖析</a> <a href="/tags/rdd计算/" style="font-size: 15px;">rdd计算</a> <a href="/tags/smoothing/" style="font-size: 15px;">smoothing</a> <a href="/tags/tensorboard/" style="font-size: 15px;">tensorboard</a> <a href="/tags/test-in-ubuntu/" style="font-size: 15px;">test in ubuntu</a> <a href="/tags/tf-nn/" style="font-size: 15px;">tf.nn</a> <a href="/tags/tensor/" style="font-size: 15px;">tensor</a> <a href="/tags/gather/" style="font-size: 15px;">gather</a> <a href="/tags/cnn/" style="font-size: 15px;">cnn</a> <a href="/tags/trie/" style="font-size: 15px;">trie</a> <a href="/tags/transformer/" style="font-size: 15px;">transformer</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/torch/" style="font-size: 15px;">torch</a> <a href="/tags/torch-geometric/" style="font-size: 15px;">torch-geometric</a> <a href="/tags/图网络/" style="font-size: 15px;">图网络</a> <a href="/tags/矩阵补全/" style="font-size: 15px;">矩阵补全</a> <a href="/tags/推荐/" style="font-size: 15px;">推荐</a> <a href="/tags/NAS/" style="font-size: 15px;">NAS</a> <a href="/tags/多任务学习/" style="font-size: 15px;">多任务学习</a> <a href="/tags/universal-embedding/" style="font-size: 15px;">universal embedding</a> <a href="/tags/迁移学习/" style="font-size: 15px;">迁移学习</a> <a href="/tags/网络修剪/" style="font-size: 15px;">网络修剪</a> <a href="/tags/知识迁移/" style="font-size: 15px;">知识迁移</a> <a href="/tags/神经架构搜索/" style="font-size: 15px;">神经架构搜索</a> <a href="/tags/async/" style="font-size: 15px;">async</a> <a href="/tags/异步/" style="font-size: 15px;">异步</a> <a href="/tags/多进程/" style="font-size: 15px;">多进程</a> <a href="/tags/流数据/" style="font-size: 15px;">流数据</a> <a href="/tags/概念漂移/" style="font-size: 15px;">概念漂移</a> <a href="/tags/二分搜索/" style="font-size: 15px;">二分搜索</a> <a href="/tags/例行化/" style="font-size: 15px;">例行化</a> <a href="/tags/逻辑/" style="font-size: 15px;">逻辑</a> <a href="/tags/机器学习工具/" style="font-size: 15px;">机器学习工具</a> <a href="/tags/github-blog/" style="font-size: 15px;">github blog</a> <a href="/tags/协同过滤/" style="font-size: 15px;">协同过滤</a> <a href="/tags/GCN/" style="font-size: 15px;">GCN</a> <a href="/tags/图卷积/" style="font-size: 15px;">图卷积</a> <a href="/tags/在线学习/" style="font-size: 15px;">在线学习</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/评估指标/" style="font-size: 15px;">评估指标</a> <a href="/tags/多标签分类/" style="font-size: 15px;">多标签分类</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">八月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">五月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">四月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">十一月 2014</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://petr-mitrichev.blogspot.com/" title="petr" target="_blank">petr</a><ul></ul><a href="http://blog.pluskid.org/" title="Free Mind" target="_blank">Free Mind</a><ul></ul><a href="http://www.flickering.cn/" title="火光摇曳" target="_blank">火光摇曳</a><ul></ul><a href="https://recsys.acm.org/" title="recsys" target="_blank">recsys</a><ul></ul><a href="https://sites.google.com/site/xreborner/" title="Xreborner" target="_blank">Xreborner</a><ul></ul><a href="http://blog.watashi.ws/" title="watashi" target="_blank">watashi</a><ul></ul><a href="https://toc.csail.mit.edu/user/306" title="WJMZBMR" target="_blank">WJMZBMR</a><ul></ul><a href="http://dongxicheng.org/" title="dongxicheng" target="_blank">dongxicheng</a><ul></ul><a href="https://www.byvoid.com/zht/" title="byvoid" target="_blank">byvoid</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p><span> Copyright &copy;<a href="/." rel="nofollow">TiuVe.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><!-- 页面点击小红心，在末尾添加，避免找不到 -->
<script type="text/javascript" src="/js/love.js"></script>
<!-- 背景彩带, true打开，false关闭 --><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script><script>(function(d, s) {
  var j, e = d.getElementsByTagName('body')[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.appendChild(j);
})(document, 'script');
</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/Epsilon2.1.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>