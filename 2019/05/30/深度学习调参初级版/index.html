<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="努力成为一名实力很强的小白"><title>深度学习调参初级版 | TiuVe</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="stylesheet" type="text/css" href="/css/donate.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深度学习调参初级版</h1><a id="logo" href="/.">TiuVe</a><p class="description">气蒸云梦泽，波撼岳阳城</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/./archives/"><i class="fa fa-archive"> 归档</i></a><a href="/./about/about.html"><i class="fa fa-user"> 关于</i></a><a href="/./project/project.html"><i class="fa fa-archive"> work</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">深度学习调参初级版</h1><div class="post-meta"><a href="/2019/05/30/深度学习调参初级版/#comments" class="comment-count"></a><p><span class="date">May 30, 2019</span><span><a href="/categories/深度学习/" class="category">深度学习</a></span></p></div><div class="post-content"><p>今天介绍一些深度学习调参的初级经验。其实知乎上已经有相关问题了，见《 <a href="https://www.zhihu.com/question/25097993" target="_blank" rel="noopener">深度学习调参有哪些技巧？</a>》，这里总结一下，并做一些补充。对于初学者来说，深度学习调参有几个比较重要的参数。学习率，损失函数，层大小，参数正则化，参数初始化的分布，优化函数，模型深度，dropout，batch大小。先引入量子位的一张图：</p>
<img src="/images/20190530_1_deeplearn_param.jpg" width="50%" height="50%">

<h3 id="1-学习率"><a href="#1-学习率" class="headerlink" title="1 学习率"></a>1 学习率</h3><p>学习率是深度学习调参中一个比较重要的参数，其决定了模型收敛的速度，以及是否可以收敛到极值。学习率很大模型收敛地很快，但是收敛到一定程度模型容易发生震荡，无法达到极值点；将学习率设置很大也容易导致loss变成Nan。如果学习率设置地很小，模型会训练地比较慢，也可能落入局部极小值。</p>
<p>一般学习率可以从0.1或0.01开始尝试。大多数情况下，使用衰减学习率有助于模型训练，在TF中实现了学习率的指数衰减函数 tf.train.exponential_decay()。函数原型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.train.exponential_decay(</span><br><span class="line">    learning_rate,    <span class="comment"># 上一轮学习率</span></span><br><span class="line">    global_step,      <span class="comment"># 自增</span></span><br><span class="line">    decay_steps,      <span class="comment"># 通常表示完整使用一遍训练数据所需要的迭代轮数,总训练样本/一个batch样本数</span></span><br><span class="line">    decay_rate,       <span class="comment"># 衰减系数</span></span><br><span class="line">    staircase=<span class="literal">False</span>,  <span class="comment"># 当staircase=True, global_step/decay_steps为整数,此时学习率变成阶梯函数</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>上述函数实现了如下功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">decayed_learning_rate = learning_rate *</span><br><span class="line">                        decay_rate ^ (global_step / decay_steps)</span><br></pre></td></tr></table></figure>

<p>举个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">starter_learning_rate = <span class="number">0.1</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,</span><br><span class="line">                                           <span class="number">100</span>, <span class="number">0.96</span>, staircase=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># Passing global_step to minimize() will increment it at each step.</span></span><br><span class="line">learning_step = (</span><br><span class="line">    tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line">    .minimize(...my loss..., global_step=global_step)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>上述代码指定了staircase&#x3D;True，所以每训练100轮后学习率乘以0.96。</p>
<h3 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2. 损失函数"></a>2. 损失函数</h3><p>TF既支持经典的损失函数，也支持自定义损失函数。一般用的交叉熵，别的损失函数没有怎么实践过。</p>
<h3 id="3-参数正则化"><a href="#3-参数正则化" class="headerlink" title="3. 参数正则化"></a>3. 参数正则化</h3><p>一般使用L2正则化对权重和偏置参数做限制，通过函数 tf.nn.l2_loss 实现。具体用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regularizers = tf.nn.l2_loss(weights[<span class="string">'h1'</span>]) + tf.nn.l2_loss(biases[<span class="string">'b1'</span>]) </span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + beta * regularizers)</span><br></pre></td></tr></table></figure>

<p>L2正则的beta参数可以设置0.0001，也可以尝试下0.001。</p>
<h3 id="4-层大小"><a href="#4-层大小" class="headerlink" title="4. 层大小"></a>4. 层大小</h3><p>知道这个有影响，但是还没有摸索出什么经验。</p>
<h3 id="5-参数初始化的分布"><a href="#5-参数初始化的分布" class="headerlink" title="5. 参数初始化的分布"></a>5. 参数初始化的分布</h3><p>参数初始化一般就是高斯分布&#x2F;均匀分布，但是在使用上述两种方式初始化时，可以采用一定的技巧，引用知乎用户<a href="https://www.zhihu.com/question/41631631" target="_blank" rel="noopener">萧瑟</a>的回答：</p>
<blockquote>
<ol>
<li>uniform均匀分布初始化：<br>w &#x3D; np.random.uniform(low&#x3D;-scale, high&#x3D;scale, size&#x3D;[n_in,n_out])</li>
</ol>
<ul>
<li>Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale &#x3D; np.sqrt(3&#x2F;n)</li>
<li>He初始化，适用于ReLU：scale &#x3D; np.sqrt(6&#x2F;n)</li>
</ul>
<ol start="2">
<li>normal高斯分布初始化：<br>w &#x3D; np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0</li>
</ol>
<ul>
<li>Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev &#x3D; np.sqrt(n)</li>
<li>He初始化，适用于ReLU：stdev &#x3D; np.sqrt(2&#x2F;n)svd初始化：对RNN有比较好的效果。参考论文：<a href="https://arxiv.org/abs/1312.6120" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6120</a></li>
</ul>
<p>Xavier初始法论文：<a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a><br>He初始化论文：<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">https://arxiv.org/abs/1502.01852</a></p>
</blockquote>
<ul>
<li>Xavier初始化<br>基本思想是：保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0</li>
<li>He初始化<br>基本思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2。 详细可以看下这篇文章《<a href="https://zhuanlan.zhihu.com/p/25110150" target="_blank" rel="noopener">聊一聊深度学习的weight initialization</a>》，讲的挺好的。</li>
</ul>
<p>大多数情况下使用上述初始化方式已足够，但是也有例外。有人使用全0初始化取得了不错的效果，所以参数初始化并没有什么固定的结论，多尝试下吧。一般来说，良好的初始化可以让参数更加逼近最优解，大大提高收敛速度，防止局部最小。</p>
<p>另外，TF的随机数生成函数如下表所示：</p>
<table>
<thead>
<tr>
<th align="center">函数名称</th>
<th align="center">随机数分布</th>
<th align="center">主要参数</th>
</tr>
</thead>
<tbody><tr>
<td align="center">tf.random_normal</td>
<td align="center">正太分布</td>
<td align="center">平均值，标准差，取值类型</td>
</tr>
<tr>
<td align="center">tf.truncated_normal</td>
<td align="center">正太分布，但如果随机出来的值偏离平均值超过2个标准差，则重新随机</td>
<td align="center">平均值，标准差，取值类型</td>
</tr>
<tr>
<td align="center">tf.random_uniform</td>
<td align="center">均匀分布</td>
<td align="center">最小、最大取值、   取值类型</td>
</tr>
<tr>
<td align="center">tf.random_gamma</td>
<td align="center">Gamma 分布</td>
<td align="center">形状参数、尺度参数beta，取值类型</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(tf.random_normal([gender_emlen], stddev=std))</span><br></pre></td></tr></table></figure>

<p>TF也支持通过常数初始化一个变量，下表是常用的常量声明方法：</p>
<table>
<thead>
<tr>
<th align="center">函数名称</th>
<th align="center">功能</th>
<th align="center">样例</th>
</tr>
</thead>
<tbody><tr>
<td align="center">tf.zeros</td>
<td align="center">产生全0的数组</td>
<td align="center">tf.zeros([2,3], int32) -&gt; [[0,0,0],[0,0,0]]</td>
</tr>
<tr>
<td align="center">tf.ones</td>
<td align="center">产生全0的数组</td>
<td align="center">tf.ones([2,3], int32) -&gt; [[1,1,1],[1,1,1]]</td>
</tr>
<tr>
<td align="center">tf.fill</td>
<td align="center">产生全为给定数字的数组</td>
<td align="center">tf.fill([2,3],9)  -&gt; [[9,9,9],[9,9,9]]</td>
</tr>
<tr>
<td align="center">tf.constant</td>
<td align="center">Gamma 分布</td>
<td align="center">tf.constant([1,2,3]  -&gt;  [1,2,3])</td>
</tr>
</tbody></table>
<h3 id="6-优化函数"><a href="#6-优化函数" class="headerlink" title="6. 优化函数"></a>6. 优化函数</h3><p>我目前使用adam函数多一些，据说adam在大多数场景表现良好。另外，sgd +momentum据说也不错？</p>
<h3 id="7-模型深度"><a href="#7-模型深度" class="headerlink" title="7. 模型深度"></a>7. 模型深度</h3><p>在工业上一般使用浅层模型，我一般尝试不超过4层网络，多了效果通常不太理想，收敛太慢。 </p>
<h3 id="8-dropout"><a href="#8-dropout" class="headerlink" title="8. dropout"></a>8. dropout</h3><p>dropout一般设置0.4-0.6，通常设置0.5，这样网络变化最大。但是也不是绝对的。</p>
<h3 id="9-batch大小"><a href="#9-batch大小" class="headerlink" title="9. batch大小"></a>9. batch大小</h3><p>之所以神经网络模型会分batch训练，是因为当数据集很大时内存放不下，而一条一条数据集进行训练会导致模型震荡不收敛，所以选择了一个折中方案，即批训练。</p>
<p><strong>增大batch的好处:</strong></p>
<ul>
<li>内存利用率提高，每个epoch训练的轮次减少，训练相同的数据量时间更短；</li>
<li>一定范围内，提高batch可以使得模型训练更稳定，不至于严重震荡(相对地，参数更新相对更慢，达到相同精度所需要的时间增加)。</li>
</ul>
<p><strong>增大batch可能引起的问题:</strong></p>
<ul>
<li>内存不够；</li>
<li>跑完一个epoch的轮次减少，达到相同精度所需要的epoch次数增加，即增加了训练时间；</li>
<li>batch增大到一定程度，其下降方向基本不再变化；</li>
<li>过大的batch会大大降低了下降的随机性，模型可能达到局部极小值，精度降低；</li>
<li>过大的batch会大大降低了下降的随机性，模型的泛化性能可能会下降，见知乎用户<strong>龙鹏-言有三</strong>的<a href="https://www.zhihu.com/question/32673260" target="_blank" rel="noopener">回答</a>。</li>
</ul>
<p>知乎用户<strong>龙鹏-言有三</strong>还给出了两个建议：</p>
<ul>
<li>如果增加了学习率，那么batch size最好也跟着增加，这样收敛更稳定。</li>
<li>尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch size，学习率对模型的收敛影响真的很大，慎重调整。</li>
</ul>
<p>对于batch大小的取值，知乎用户<a href="https://zhuanlan.zhihu.com/p/27763696" target="_blank" rel="noopener">夕小瑶</a>也给出的自己的经验：</p>
<blockquote>
<p>对于SGD（随机梯度下降）及其改良的一阶优化算法如Adagrad、Adam等是没问题的，但是对于强大的二阶优化算法如共轭梯度法、L-BFGS来说，如果估计不好一阶导数，那么对二阶导数的估计会有更大的误差，这对于这些算法来说是致命的。</p>
<p>因此，对于二阶优化算法，减小batch换来的收敛速度提升远不如引入大量噪声导致的性能下降，因此在使用二阶优化算法时，往往要采用大batch哦。此时往往batch设置成几千甚至一两万才能发挥出最佳性能。</p>
<p>另外，听说GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优（不过我没有验证过，有兴趣的同学可以试验一下~）</p>
</blockquote>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay" target="_blank" rel="noopener">tf.train.exponential_decay</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25110150" target="_blank" rel="noopener">聊一聊深度学习的weight initialization</a></li>
<li><a href="https://www.zhihu.com/question/41631631" target="_blank" rel="noopener">你有哪些deep learning（rnn、cnn）调参的经验？</a></li>
<li><a href="https://www.zhihu.com/question/25097993" target="_blank" rel="noopener">深度学习调参有哪些技巧？</a></li>
<li><a href="https://www.zhihu.com/question/32673260" target="_blank" rel="noopener">深度学习中的batch的大小对学习效果有何影响？龙鹏-言有三</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27763696" target="_blank" rel="noopener">训练神经网络时如何确定batch size？夕小瑶</a></li>
</ol>
</div><div class="tags"><a href="/tags/深度学习/">深度学习</a><a href="/tags/调参/">调参</a></div><script type="text/javascript" src="/js/jquery.js?v=2.0.1" async></script><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" href="javascript:;" title="打赏" class="btn_donate"></a><div class="donate_txt"> &uarr;<br>谢谢~ 您的支持将鼓励我继续创作！<br></div></div><div id="donate_guide" class="donate_bar center hidden pay"><img src="/img/weChatMoney.png" title="微信打赏" alt="微信打赏"><img src="/img/alipayMoney.png" title="支付宝打赏" alt="支付宝打赏"></div><script type="text/javascript">document.getElementById('btn_donate').onclick = function(){
    $('#donate_board').addClass('hidden');
    $('#donate_guide').removeClass('hidden');
}</script></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2019/07/11/centos7安装zookeeper集群/" class="pre">centos7安装zookeeper集群</a><a href="/2019/05/28/记一次深度学习实践之空间复杂度的坑/" class="next">记一次深度学习实践之空间复杂度的坑</a></div><div id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80MTIyOC8xNzc3Ng=="></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-学习率"><span class="toc-text">1 学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-损失函数"><span class="toc-text">2. 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-参数正则化"><span class="toc-text">3. 参数正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-层大小"><span class="toc-text">4. 层大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-参数初始化的分布"><span class="toc-text">5. 参数初始化的分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-优化函数"><span class="toc-text">6. 优化函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-模型深度"><span class="toc-text">7. 模型深度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-dropout"><span class="toc-text">8. dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-batch大小"><span class="toc-text">9. batch大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/03/29/《Contrastive-Learning-for-Cold-Start-Recommendation》阅读笔记/">《Contrastive Learning for Cold-Start Recommendation》阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/09/07/《KDD2018-Perceive-Your-Users-in-Depth-Learning-Universal-User-Representations-from-Multiple-E-commerce-Tasks》阅读笔记/">《KDD2018 Perceive Your Users in Depth Learning Universal User Representations from Multiple E-commerce Tasks》阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/10/《Parameter-Efficient-Transfer-from-Sequential-Behaviors-for-User-Modeling-and-Recommendation》论文阅读/">《Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation》论文阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/03/《Towards-Universal-Sequence-Representation-Learning-for-Recommender-Systems》论文阅读笔记/">《Towards Universal Sequence Representation Learning for Recommender Systems》论文阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/07/NLP相关资料整理/">NLP相关资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/hexo博客链接在微信被屏蔽的解决办法/">hexo博客链接在微信被屏蔽的解决办法</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/TensorFlow的自动求导具体是在哪部分代码里实现的？/">TensorFlow的自动求导具体是在哪部分代码里实现的？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/tf中如何修改tensor的值/">tf中如何修改tensor的值</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/04/解决tf1-15中tf-scatter-update-函数没有定义梯度的问题/">解决tf1.15中tf.scatter_update()函数没有定义梯度的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/31/解决hexo博客代码在IOS下字体过大的问题/">解决hexo博客代码在IOS下字体过大的问题</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DMMLAI/">DMMLAI</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo主题/">Hexo主题</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hive/">Hive</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PL/">PL</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hole/">hole</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/mac/">mac</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sklearn/">sklearn</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/例行化/">例行化</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发/">开发</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发工具/">开发工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐/">推荐</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/摄影/">摄影</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据获取/">数据获取</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/游记/">游记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学/">统计学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计划/">计划</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算广告/">计算广告</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发/">高并发</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/cold/" style="font-size: 15px;">cold</a> <a href="/tags/gwen/" style="font-size: 15px;">gwen</a> <a href="/tags/C-C/" style="font-size: 15px;">C/C++</a> <a href="/tags/FTRL/" style="font-size: 15px;">FTRL</a> <a href="/tags/hive/" style="font-size: 15px;">hive</a> <a href="/tags/Java编程思想/" style="font-size: 15px;">Java编程思想</a> <a href="/tags/kmp/" style="font-size: 15px;">kmp</a> <a href="/tags/字符串匹配/" style="font-size: 15px;">字符串匹配</a> <a href="/tags/L2范数，范数/" style="font-size: 15px;">L2范数，范数</a> <a href="/tags/linux性能管理/" style="font-size: 15px;">linux性能管理</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/自动求导，自动微分/" style="font-size: 15px;">自动求导，自动微分</a> <a href="/tags/sublime/" style="font-size: 15px;">sublime</a> <a href="/tags/TFRecord/" style="font-size: 15px;">TFRecord</a> <a href="/tags/FM/" style="font-size: 15px;">FM</a> <a href="/tags/指标/" style="font-size: 15px;">指标</a> <a href="/tags/auc/" style="font-size: 15px;">auc</a> <a href="/tags/zookepper/" style="font-size: 15px;">zookepper</a> <a href="/tags/centos7/" style="font-size: 15px;">centos7</a> <a href="/tags/cpm/" style="font-size: 15px;">cpm</a> <a href="/tags/opcm/" style="font-size: 15px;">opcm</a> <a href="/tags/ecpm/" style="font-size: 15px;">ecpm</a> <a href="/tags/hexo，微信屏蔽/" style="font-size: 15px;">hexo，微信屏蔽</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/tf/" style="font-size: 15px;">tf</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/重装系统/" style="font-size: 15px;">重装系统</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/OJ/" style="font-size: 15px;">OJ</a> <a href="/tags/dfs/" style="font-size: 15px;">dfs</a> <a href="/tags/递归/" style="font-size: 15px;">递归</a> <a href="/tags/poj/" style="font-size: 15px;">poj</a> <a href="/tags/回文字符串/" style="font-size: 15px;">回文字符串</a> <a href="/tags/dp/" style="font-size: 15px;">dp</a> <a href="/tags/暴力/" style="font-size: 15px;">暴力</a> <a href="/tags/棋盘规则/" style="font-size: 15px;">棋盘规则</a> <a href="/tags/状态压缩/" style="font-size: 15px;">状态压缩</a> <a href="/tags/BFS/" style="font-size: 15px;">BFS</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/pytorch/" style="font-size: 15px;">pytorch</a> <a href="/tags/module/" style="font-size: 15px;">module</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/shell/" style="font-size: 15px;">shell</a> <a href="/tags/质数/" style="font-size: 15px;">质数</a> <a href="/tags/素数/" style="font-size: 15px;">素数</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/hole/" style="font-size: 15px;">hole</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/打散/" style="font-size: 15px;">打散</a> <a href="/tags/roc-auc-score/" style="font-size: 15px;">roc_auc_score</a> <a href="/tags/svm多类别分类/" style="font-size: 15px;">svm多类别分类</a> <a href="/tags/网格搜索/" style="font-size: 15px;">网格搜索</a> <a href="/tags/textFile/" style="font-size: 15px;">textFile</a> <a href="/tags/源码剖析/" style="font-size: 15px;">源码剖析</a> <a href="/tags/rdd计算/" style="font-size: 15px;">rdd计算</a> <a href="/tags/smoothing/" style="font-size: 15px;">smoothing</a> <a href="/tags/tensorboard/" style="font-size: 15px;">tensorboard</a> <a href="/tags/test-in-ubuntu/" style="font-size: 15px;">test in ubuntu</a> <a href="/tags/tf-nn/" style="font-size: 15px;">tf.nn</a> <a href="/tags/tensor/" style="font-size: 15px;">tensor</a> <a href="/tags/gather/" style="font-size: 15px;">gather</a> <a href="/tags/cnn/" style="font-size: 15px;">cnn</a> <a href="/tags/transformer/" style="font-size: 15px;">transformer</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a> <a href="/tags/trie/" style="font-size: 15px;">trie</a> <a href="/tags/推荐，对比学习，冷启动/" style="font-size: 15px;">推荐，对比学习，冷启动</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/torch/" style="font-size: 15px;">torch</a> <a href="/tags/torch-geometric/" style="font-size: 15px;">torch-geometric</a> <a href="/tags/多任务学习/" style="font-size: 15px;">多任务学习</a> <a href="/tags/图网络/" style="font-size: 15px;">图网络</a> <a href="/tags/矩阵补全/" style="font-size: 15px;">矩阵补全</a> <a href="/tags/推荐/" style="font-size: 15px;">推荐</a> <a href="/tags/NAS/" style="font-size: 15px;">NAS</a> <a href="/tags/网络修剪/" style="font-size: 15px;">网络修剪</a> <a href="/tags/知识迁移/" style="font-size: 15px;">知识迁移</a> <a href="/tags/神经架构搜索/" style="font-size: 15px;">神经架构搜索</a> <a href="/tags/async/" style="font-size: 15px;">async</a> <a href="/tags/异步/" style="font-size: 15px;">异步</a> <a href="/tags/多进程/" style="font-size: 15px;">多进程</a> <a href="/tags/流数据/" style="font-size: 15px;">流数据</a> <a href="/tags/概念漂移/" style="font-size: 15px;">概念漂移</a> <a href="/tags/二分搜索/" style="font-size: 15px;">二分搜索</a> <a href="/tags/univesal-embedding/" style="font-size: 15px;">univesal embedding</a> <a href="/tags/机器学习工具/" style="font-size: 15px;">机器学习工具</a> <a href="/tags/universal-embedding/" style="font-size: 15px;">universal embedding</a> <a href="/tags/迁移学习/" style="font-size: 15px;">迁移学习</a> <a href="/tags/逻辑/" style="font-size: 15px;">逻辑</a> <a href="/tags/github-blog/" style="font-size: 15px;">github blog</a> <a href="/tags/例行化/" style="font-size: 15px;">例行化</a> <a href="/tags/协同过滤/" style="font-size: 15px;">协同过滤</a> <a href="/tags/GCN/" style="font-size: 15px;">GCN</a> <a href="/tags/图卷积/" style="font-size: 15px;">图卷积</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/在线学习/" style="font-size: 15px;">在线学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">九月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">八月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">七月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">五月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">四月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">十一月 2014</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://petr-mitrichev.blogspot.com/" title="petr" target="_blank">petr</a><ul></ul><a href="http://blog.pluskid.org/" title="Free Mind" target="_blank">Free Mind</a><ul></ul><a href="http://www.flickering.cn/" title="火光摇曳" target="_blank">火光摇曳</a><ul></ul><a href="https://recsys.acm.org/" title="recsys" target="_blank">recsys</a><ul></ul><a href="https://sites.google.com/site/xreborner/" title="Xreborner" target="_blank">Xreborner</a><ul></ul><a href="http://blog.watashi.ws/" title="watashi" target="_blank">watashi</a><ul></ul><a href="https://toc.csail.mit.edu/user/306" title="WJMZBMR" target="_blank">WJMZBMR</a><ul></ul><a href="http://dongxicheng.org/" title="dongxicheng" target="_blank">dongxicheng</a><ul></ul><a href="https://www.byvoid.com/zht/" title="byvoid" target="_blank">byvoid</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p><span> Copyright &copy;<a href="/." rel="nofollow">TiuVe.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><!-- 页面点击小红心，在末尾添加，避免找不到 -->
<script type="text/javascript" src="/js/love.js"></script>
<!-- 背景彩带, true打开，false关闭 --><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script><script>(function(d, s) {
  var j, e = d.getElementsByTagName('body')[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.appendChild(j);
})(document, 'script');
</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/Epsilon2.1.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>