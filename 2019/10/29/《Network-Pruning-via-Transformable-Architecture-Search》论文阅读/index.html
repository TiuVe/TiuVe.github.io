<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="努力成为一名实力很强的小白"><title>《Network Pruning via Transformable Architecture Search》论文阅读 | TiuVe</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="stylesheet" type="text/css" href="/css/donate.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">《Network Pruning via Transformable Architecture Search》论文阅读</h1><a id="logo" href="/.">TiuVe</a><p class="description">气蒸云梦泽，波撼岳阳城</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/./archives/"><i class="fa fa-archive"> 归档</i></a><a href="/./about/about.html"><i class="fa fa-user"> 关于</i></a><a href="/./project/project.html"><i class="fa fa-archive"> work</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">《Network Pruning via Transformable Architecture Search》论文阅读</h1><div class="post-meta"><a href="/2019/10/29/《Network-Pruning-via-Transformable-Architecture-Search》论文阅读/#comments" class="comment-count"></a><p><span class="date">Oct 29, 2019</span><span><a href="/categories/推荐/" class="category">推荐</a></span></p></div><div class="post-content"><p>网络修剪可减少过度参数化的网络的计算成本，而不会影响性能。现有的修剪算法会预先定义修剪网络的宽度和深度，然后将参数从未修剪的网络传输到修剪的网络。为了突破修剪网络的结构限制，我们提出应用神经架构搜索（NAS）直接搜索具有灵活的通道大小和层大小的网络。通过最小化修剪网络的损失来学习通道&#x2F;层的数量。修剪后的网络的特征图是K个特征图片段的集合（由不同大小的K个网络生成），这些片段是根据概率分布进行采样的。损耗不仅可以反向传播到网络权重，还可以反向传播到参数化分布，以显式调整通道&#x2F;层的大小。具体来说，我们应用逐通道插值（填充）以使具有不同通道大小的特征图在聚合过程中保持对齐。每个分布中的size的最大概率用作修剪网络的宽度和深度，修剪网络的参数是通过知识迁移（例如知识蒸馏）从原始网络中获知的。与传统的网络修剪算法相比，在CIFAR-10，CIFAR-100和ImageNet上进行的实验证明了我们的网络修剪新观点的有效性。进行了各种搜索和知识转移方法以显示这两个组件的有效性。代码位于：<a href="https://github.com/D-X-Y/NAS-Projects" target="_blank" rel="noopener">https://github.com/D-X-Y/NAS-Projects</a></p>
<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h3><p>深度卷积神经网络（CNN）正在变得更宽更深，以在不同的应用程序上实现高性能[17、22、48]。 尽管它们取得了巨大的成功，但将它们部署到资源受限的设备（如移动设备和无人机）上并不可行。 解决该问题的一个直接方案是使用网络修剪[29、12、13、20、18]来减少过参数化CNN的计算成本。 如图1(a)所示，用于网络修剪的典型pipeline是通过删除冗余过滤器，然后基于原始网络微调修剪网络来实现的。基于滤波器重要性的不同技巧被使用，例如滤波器的L2范数[30]，重构误差[20]和可学习的缩放因子[32]。最后，研究人员对修剪后的网络应用了各种微调策略[30，18]，以有效地传递未修剪网络的参数并最大化修剪后的网络的性能。</p>
<img src="/images/201910301224_0.png" width="50%" height="50%">

<p>传统的网络修剪方法在保持准确性的同时，对网络进行了有效地压缩。它们的网络结构是直观设计的，例如，在每一层中裁减30％的滤波器[30、18]，预测稀疏率[15]或利用正则化[2]。修剪后的网络精度受限于人工设计的结构或结构规则。为了克服这个限制，我们使用神经架构搜索（NAS）将架构设计转变为学习过程，并提出了一种新的网络修剪范例，如图1(b)所示。</p>
<p>现有的NAS方法[31、48、8、4、40]优化了网络拓扑，而本文的重点是自动网络规模。为了满足要求并公平地比较以前的修剪策略，我们提出了一种称为可迁移架构搜索（TAS）的新NAS方案。**<font color="#dd0000"> TAS旨在搜索最佳网络规模而不是拓扑，通过最小化计算成本进行正则化（浮点操作FLOPs）。然后，通过知识迁移来学习搜索&#x2F;修剪网络的参数</font>**[21、44、46]。 </p>
<p>TAS是一种可微分的搜索算法，可以有效且高效地搜索网络的宽度和深度。具体而言，不同的候选通道&#x2F;层以可学习的概率被添加到网络中。通过反向传播修剪网络产生的损失来学习概率分布，修剪后网络的特征图是根据概率分布采样的K个特征图片段（不同大小的网络输出）的集合。这些不同通道大小的特征图借助通道插值方式进行汇总。每个分布中概率最大的size用作修剪网络的宽度和深度。</p>
<p>在实验中，我们证明，通过知识蒸馏（KD）传递参数的搜索架构优于之前在CIFAR-10，CIFAR-100和ImageNet上的修剪方法。我们还对传统的人工修剪方法[30，18]和随机架构搜索方法[31]生成的架构测试了不同的知识迁移方法。对不同架构的相同改进证明了知识迁移的普遍性。</p>
<h3 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h3><p>网络修剪[29，33]是压缩和加速CNN的有效技术，因此允许我们在存储和计算资源有限的硬件设备上部署有效的网络。已经提出了多种技术，例如低秩分解[47]，权重修剪[14、29、13、12]，通道修剪[18、33]，动态计算[9、7]和量化[23， 1]。它们有两种模式：非结构化修剪[29、9、7、12]和结构化修剪[30、20、18、33]。</p>
<p><strong>非结构化修剪方法</strong>[29、9、7、12]通常会 **<font color="#0000FF">强制卷积权重[29、14]或特征图[7、9]稀疏</font>**。非结构化修剪的先驱 LeCun等[29]和Hassibi等[14]研究了使用二阶导数信息来修剪浅层CNN的权重。在深度网络于2012年诞生后[28]，Han等人[12，13，11]提出了一系列基于L2正则化获得高度压缩的深度CNN的工作。经过这一发展，许多研究人员探索了不同的正则化技术来提高稀疏度，同时又保持了准确性，例如L0正则化[35]和输出灵敏度[41]。由于这些非结构化方法使大型网络稀疏而不是改变网络的整体结构，因此它们需要针对依赖项的专用设计[11]和特定的硬件来加快推理过程。</p>
<p><strong>结构化修剪方法</strong>[30、20、18、33]的目标是 **<font color="#0000FF">对卷积过滤器或所有层进行修剪</font>**，因此可以轻松开发和应用修剪后的网络。该领域的早期工作[2，42]利用组Lasso来实现深度网络的结构化稀疏性。之后，李等人[30]提出了典型的三阶段修剪范例（训练大型网络，修剪，再训练）。这些修剪算法将具有较小范数的过滤器视为不重要，并且倾向于修剪它们，但是这种假设在深层非线性网络中不成立[43]。因此，许多研究人员专注于信息过滤器的更好标准。例如，刘等[32]利用L1正则化；Ye等[43]对ISTA施加了惩罚(applied a ISTA penalty)；He等[19]利用了基于几何中位数的标准。与以前的修剪pipeline相反，我们的方法允许显式优化通道&#x2F;层的数量，从而使学习到的结构具有高性能和低成本。</p>
<p>除了信息过滤器的标准，网络结构的重要性在[33]中被提出。通过自动确定每一层的修剪和压缩率，某些方法可以隐式地找到特定于数据的架构[42、2、15]。相比之下，我们使用NAS明确发现了该架构。先前的大多数NAS算法[48、8、31、40]会自动发现神经网络的拓扑结构，而我们专注于搜索神经网络的深度和宽度。基于强化学习（RL）的[48，3]方法或基于进化算法的[40]方法可以搜索具有灵活宽度和深度的网络，但是它们需要大量的计算资源，因此无法直接用于大规模目标数据集。可微分的方法[8，31，4]显着降低了计算成本，但它们通常假定不同搜索候选集的通道数相同。 TAS是一种可微分的NAS方法，它能够有效地搜索具有灵活宽度和深度的迁移网络。</p>
<p>网络迁移(Network transformation)[5，10，3]也研究了网络的深度和宽度。 Chen等[5]手动拓宽和加深网络，并建议使用Net2Net初始化更大的网络。 Ariel等[10]提出了一种启发式策略，通过在缩小和扩展之间交替来找到合适的网络宽度。蔡等[3]利用RL代理来增加CNN的深度和宽度，而我们的TAS是一种可微分的方法，不仅可以扩大CNN，而且可以缩小CNN。</p>
<p>在网络修剪相关的文献中，知识迁移已被证明是有效的。网络的参数可以从预训练的初始化[30，18]中迁移。Minnehan等[37]通过逐块重构损失来迁移未压缩网络的知识。在本文中，我们应用了一种简单的KD方法[21]来进行知识转移，从而使架构搜索具有良好的性能。</p>
<h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3 方法"></a>3 方法</h3><p>我们的修剪方法包括三个步骤：（1）通过标准分类训练程序训练未修剪的大型网络。 （2）通过TAS搜索小型网络的深度和宽度。（3）通过简单的KD方法将知识从未修剪的大型网络迁移到搜索得到的小型网络[21]。 下面将介绍背景知识，TAS的详细信息，并说明知识迁移过程。</p>
<h4 id="3-1-迁移架构搜索"><a href="#3-1-迁移架构搜索" class="headerlink" title="3.1 迁移架构搜索"></a>3.1 迁移架构搜索</h4><p>网络通道修剪旨在减少网络每一层中的通道数量。 给定输入图像，网络会将其作为输入，并在每个目标类别上产生概率。 假设$X$和$O$是第$l$个卷积层的输入和输出特征张量（我们以3×3卷积为例），该层计算过程如下：</p>
<p>$$<br>O_j&#x3D;\sum_{k&#x3D;1}^{c_{in}}{X_k,:,: \ast W_{j,k},:,:}  \qquad  where  ; 1\leq j \leq c_{out} \tag{1}<br>$$</p>
<p>其中$W\in R^{c_{out\times c_{in} \times 3 \times 3}}$表示卷积核权重，$c_{in}$为输入通道，$c_{out}$为输出通道。 $W_{j,k,:,:}$对应第$k$个输入通道和第$j$个输出通道。$\ast$表示卷积运算。 通道修剪方法可以减少$c_{out}$的数量，因此，也减少了下一层的$c_{in}$。</p>
<p><strong>搜索宽度</strong>  我们使用参数$\alpha \in R^{|C|}$来表示一层中可能的通道数分布，其中，$max(C)≤c_{out}$。 选择通道数量的第$j$个候选的概率可以表示为：</p>
<p>$$<br>p_j&#x3D;\frac{exp(\alpha_j)}{\sum_{k&#x3D;1}^{|C|}{exp(\alpha_k)}} \qquad where; 1\leq j \leq |C| \tag{2}<br>$$</p>
<p>但是，**<font color="#B22222">上述过程中的采样操作是不可微的</font>**（可以参考《<a href="https://blog.csdn.net/weixin_40255337/article/details/83303702" target="_blank" rel="noopener">Gumbel-Softmax的采样技巧</a>》），这阻止了我们将梯度从$p_j$反向传播到$\alpha_j$。 受[8]的启发，我们应用Gumbel-Softmax [26，36]来软化采样过程以优化$\alpha$：</p>
<p><img src="/images/20191029210453_0.png"></p>
<p>其中$U(0,1)$表示0和1之间的均匀分布。$\tau$是softmax温度参数。 当$\tau \to 0$时，$\hat{p}&#x3D;[\hat{p}_1,…,\hat{p}_j,…]$变为one-shot?（此处暂时没想明白，可以参考《<a href="https://zhuanlan.zhihu.com/p/50065712" target="_blank" rel="noopener">Gumbel-Softmax 对离散变量再参数化</a>》），并且基于$\hat{p}$的Gumbel-softmax分布与类别分布相同。当$\tau \to \infty$，Gumbel-softmax分布在$C$上变为均匀分布。我们方法中的特征图定义为具有不同大小的原始特征图片段的加权和，其中权重为$\hat{p}$。 通过逐通道插值（CWI）对齐具有不同大小的特征图，以便计算加权和。 为了减少内存成本，我们选择索引$I\subseteq[|C|]$为的小子集进行聚合，而不是使用所有channel候选集（ **<font color="dd0000">此处提出了针对形如等式2中的概率计算不可微问题可以采取的策略，即使用Gumbel-Softmax进行软化 </font>**）。此外，权重根据所选size的概率重新归一化，公式如下：</p>
<p><img src="/images/20191030026_0.png"></p>
<p>其中$\tau_{\hat{p}}$表示由$\hat{p}$参数化的多项式概率分布。CWI是将不同尺寸的特征图对齐的一般操作。它可以通过多种方式实现，例如空间迁移网络（ spatial transformer network）的3D变体[25]或自适应池操作[16]。在本文中，我们选择3D自适应平均池化操作[16]作为${CWI}^2$，因为它没有带来额外的参数并且可以忽略不计的额外成本。我们在CWI之前使用批规范化[24]来规范化不同的片段。图2以$|I|&#x3D;2$为例阐述了上述过程。</p>
<p><img src="/images/201910301502_0.png"></p>
<p><strong>需要注意的是</strong>，$I&#x3D;2$表示有两个 channel 值不同的卷积核进行CWI。具体做法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.avg : out = self.avg( inputs )</span><br><span class="line"><span class="keyword">else</span>        : out = inputs  <span class="comment">#走这个分支</span></span><br><span class="line"><span class="comment"># convolutional layer</span></span><br><span class="line">out_convs = conv_forward(out, self.conv, [self.choices[i] <span class="keyword">for</span> i <span class="keyword">in</span> index])  <span class="comment"># nn.Conv2d</span></span><br><span class="line">out_bns   = [self.BNs[idx](out_conv) <span class="keyword">for</span> idx, out_conv <span class="keyword">in</span> zip(index, out_convs)]</span><br><span class="line"><span class="comment"># merge</span></span><br><span class="line">out_channel = max([x.size(<span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> out_bns])   <span class="comment"># 因为 I=2，这里取最大的size作为CWI的对齐标准</span></span><br><span class="line">outA = ChannelWiseInter(out_bns[<span class="number">0</span>], out_channel) <span class="comment"># out_channel作为最大的size</span></span><br><span class="line">outB = ChannelWiseInter(out_bns[<span class="number">1</span>], out_channel)</span><br><span class="line">out  = outA * prob[<span class="number">0</span>] + outB * prob[<span class="number">1</span>] <span class="comment"># 对应公式4，因为设置I=2比较小，所以这里直接写死了</span></span><br></pre></td></tr></table></figure>

<p>而conv_forward中是怎么实现的呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(inputs, conv, choices)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  :param inputs: 输入数据</span></span><br><span class="line"><span class="string">  :param conv: 一个定义的卷积层</span></span><br><span class="line"><span class="string">  :param choices: 经过选择的输出channel集合，I=2则 channel size=2</span></span><br><span class="line"><span class="string">  :return: </span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  iC = conv.in_channels</span><br><span class="line">  fill_size = list(inputs.size())</span><br><span class="line">  fill_size[<span class="number">1</span>] = iC - fill_size[<span class="number">1</span>]</span><br><span class="line">  filled  = torch.zeros(fill_size, device=inputs.device)</span><br><span class="line">  xinputs = torch.cat((inputs, filled), dim=<span class="number">1</span>)</span><br><span class="line">  outputs = conv(xinputs)</span><br><span class="line">  <span class="comment">#Conv2的输出是[batch_size, out, height, width],所以outputs[:,:oC]中，逗号之前表示batch, 逗号之后表示</span></span><br><span class="line">  selecteds = [outputs[:,:oC] <span class="keyword">for</span> oC <span class="keyword">in</span> choices]</span><br><span class="line">  <span class="keyword">return</span> selecteds</span><br></pre></td></tr></table></figure>

<p>看15行，也就是说先按照候选 channel 的最大值进行卷积，然后根据候选 channel 使用切片进行获取。这样就金额以方便地获取不同 channel 对应的卷积输出。</p>
<p><code>讨论等式4中的抽样策略</code> $\quad$ 该策略旨在仅仅通过反向传播采样架构的梯度（而不是整个架构），将内存成本和训练时间减少到可接受的数量。与通过均匀分布采样相比，所应用的采样方法（基于概率的采样）可以减弱多次迭代后每次迭代采样所导致的梯度差。</p>
<p><strong>搜索深度</strong> 我们使用参数来表示具有$L$个卷积层的网络中可能的层数分布。我们使用和等式3类似的策略来采样层数。并使用深度为$l$的采样分布$\hat{q}_l$使$\beta$与$\alpha$可微分。然后，对于所有可能的深度，我们计算出修剪后的网络的最终输出特征，进行汇总，其表示为：</p>
<img src="/images/201910300037_0.png" width="80%" height="80%">

<p>其中$\hat{O^l}$表示在等式4中第$l$层的输出特征图。$C_{out}$表示所有$\hat{O^l}$中的最大采样通道。 将最终输出的特征图$O_{out}$喂入最后的分类层以进行预测。 这样，我们可以将梯度反向传播到宽度参数$\alpha$和深度参数$\beta$。</p>
<p><strong>搜索目标</strong> 最终的架构$A$是通过选择具有最大概率的候选项而得出的，该候选项由架构参数(等式8中形如$A$的字符即表示架构参数，此处不知如何书写)获知，该架构参数由每层的$\alpha$和$\beta$组成。 我们的TAS的目标是通过将训练损失$\zeta_{train}$最小化，找到经过训练后具有最小验证损失$\zeta_{val}$的架构$A$：</p>
<p><img src="/images/201910300049_0.png"></p>
<p>其中$w_A^{\ast}$表示$A$的优化权重。训练损失是网络的交叉熵分类损失。 现有的NAS方法[31、48、8、4、40]通过具有不同拓扑的网络候选项优化$A$，而我们的TAS则是搜索具有相同拓扑结构以及较小宽度和深度的候选项。 所以，我们搜索过程中的验证损失不仅包括分类验证损失，还包括计算成本的惩罚：</p>
<p><img src="/images/201910300052_0.png"></p>
<p>其中$z$是一个向量，表示修剪后网络的输出对数，$y$表示相应输入的真实类别，而$\lambda_{cost}$是$\zeta_{cost}$的权重。 成本损失会促使网络的计算成本（例如FLOP）收敛到目标R，以便可以通过设置不同的$R$来动态调整成本。我们使用的分段计算成本损失如下：</p>
<p><img src="/images/201910300055_0.png"></p>
<p>其中$E_{cost}(A)$基于架构参数$A$计算期望的计算成本。具体地说，它是所有候选网络的计算成本的加权总和，其中权重是采样概率。$F_{cost}(A)$表示搜索到的架构的实际成本，其宽度和深度从$A$得出。$t\in [0,1]$表示容忍度，它减慢了更改搜索架构的速度。需要注意的是，我们使用FLOP来评估网络的计算成本，并且很容易用其它度量标准（例如延迟[4]）来代替FLOP。</p>
<p>我们在Alg1中展示了整个算法。在搜索过程中，我们使用等式5迁移网络，使权重和架构参数可区分。我们也可以在训练集上最小化$\zeta_{train}$来优化修剪的网络的权重，而在验证集上最小化$\zeta_{val}$来优化架构参数$A$。搜索之后，我们以最大的概率选择通道数作为宽度，以最大概率选择层数作为深度。最终搜索的网络由选定的宽度和深度构成。该网络将通过KD进行优化，我们将在3.2小结中详细介绍。</p>
<img src="/images/201910300059_0.png" width="46%" height="46%">

<h4 id="3-2-知识迁移"><a href="#3-2-知识迁移" class="headerlink" title="3.2 知识迁移"></a>3.2 知识迁移</h4><p>知识迁移对于学习健壮的修剪网络非常重要，我们在搜索的网络架构上采用了简单的KD算法[21]。 此算法鼓励小型网络的预测$z$通过以下目标，与未修剪网络的软目标匹配：</p>
<p><img src="/images/201910300101_0.png"></p>
<p>其中$T$是温度参数，$\hat{z}$表示来自预训练的未修剪网络的logit输出向量。 此外，它使用带有交叉熵损失的softmax来鼓励小型网络预测真实目标。 KD的最终目标如下：</p>
<p><img src="/images/201910300102_0.png"></p>
<p>其中$y$表示相应输入的真实目标类别。$\lambda$是平衡标准分类损失和软匹配损失的损失权重。 在搜索到目标网络（第3.1节）之后，我们首先对未修剪的网络进行预训练，然后通过等式10从未修剪的网络迁移来优化搜索到的网络。</p>
<p>实验部分见下文。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/50065712" target="_blank" rel="noopener">Gumbel-Softmax 对离散变量再参数化</a></li>
<li><a href="https://blog.csdn.net/weixin_40255337/article/details/83303702" target="_blank" rel="noopener">Gumbel-Softmax的采样技巧</a></li>
</ul>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</div><div class="tags"><a href="/tags/NAS/">NAS</a><a href="/tags/网络修剪/">网络修剪</a><a href="/tags/知识迁移/">知识迁移</a><a href="/tags/神经架构搜索/">神经架构搜索</a></div><script type="text/javascript" src="/js/jquery.js?v=2.0.1" async></script><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" href="javascript:;" title="打赏" class="btn_donate"></a><div class="donate_txt"> &uarr;<br>谢谢~ 您的支持将鼓励我继续创作！<br></div></div><div id="donate_guide" class="donate_bar center hidden pay"><img src="/img/weChatMoney.png" title="微信打赏" alt="微信打赏"><img src="/img/alipayMoney.png" title="支付宝打赏" alt="支付宝打赏"></div><script type="text/javascript">document.getElementById('btn_donate').onclick = function(){
    $('#donate_board').addClass('hidden');
    $('#donate_guide').removeClass('hidden');
}</script></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2019/11/02/已悟/" class="pre">已悟</a><a href="/2019/10/20/论文《A-Simple-Convolutional-Generative-Network-for-Next-Item》阅读笔记/" class="next">论文《A Simple Convolutional Generative Network for Next Item》阅读笔记</a></div><div id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80MTIyOC8xNzc3Ng=="></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-介绍"><span class="toc-text">1 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-相关工作"><span class="toc-text">2 相关工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-方法"><span class="toc-text">3 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-迁移架构搜索"><span class="toc-text">3.1 迁移架构搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-知识迁移"><span class="toc-text">3.2 知识迁移</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/03/29/《Contrastive-Learning-for-Cold-Start-Recommendation》阅读笔记/">《ACM MM2021 Contrastive Learning for Cold-Start Recommendation》阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/09/07/《KDD2018-Perceive-Your-Users-in-Depth-Learning-Universal-User-Representations-from-Multiple-E-commerce-Tasks》阅读笔记/">《KDD2018 Perceive Your Users in Depth Learning Universal User Representations from Multiple E-commerce Tasks》阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/10/《Parameter-Efficient-Transfer-from-Sequential-Behaviors-for-User-Modeling-and-Recommendation》论文阅读/">《Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation》论文阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/03/《Towards-Universal-Sequence-Representation-Learning-for-Recommender-Systems》论文阅读笔记/">《Towards Universal Sequence Representation Learning for Recommender Systems》论文阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/07/NLP相关资料整理/">NLP相关资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/hexo博客链接在微信被屏蔽的解决办法/">hexo博客链接在微信被屏蔽的解决办法</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/TensorFlow的自动求导具体是在哪部分代码里实现的？/">TensorFlow的自动求导具体是在哪部分代码里实现的？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/05/tf中如何修改tensor的值/">tf中如何修改tensor的值</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/01/04/解决tf1-15中tf-scatter-update-函数没有定义梯度的问题/">解决tf1.15中tf.scatter_update()函数没有定义梯度的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/31/解决hexo博客代码在IOS下字体过大的问题/">解决hexo博客代码在IOS下字体过大的问题</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DMMLAI/">DMMLAI</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo主题/">Hexo主题</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hive/">Hive</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PL/">PL</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hole/">hole</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/mac/">mac</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sklearn/">sklearn</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/例行化/">例行化</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发/">开发</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/开发工具/">开发工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐/">推荐</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/摄影/">摄影</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据获取/">数据获取</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/游记/">游记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学/">统计学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计划/">计划</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算广告/">计算广告</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发/">高并发</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/cold/" style="font-size: 15px;">cold</a> <a href="/tags/gwen/" style="font-size: 15px;">gwen</a> <a href="/tags/C-C/" style="font-size: 15px;">C/C++</a> <a href="/tags/FTRL/" style="font-size: 15px;">FTRL</a> <a href="/tags/hive/" style="font-size: 15px;">hive</a> <a href="/tags/Java编程思想/" style="font-size: 15px;">Java编程思想</a> <a href="/tags/kmp/" style="font-size: 15px;">kmp</a> <a href="/tags/字符串匹配/" style="font-size: 15px;">字符串匹配</a> <a href="/tags/L2范数，范数/" style="font-size: 15px;">L2范数，范数</a> <a href="/tags/linux性能管理/" style="font-size: 15px;">linux性能管理</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/自动求导，自动微分/" style="font-size: 15px;">自动求导，自动微分</a> <a href="/tags/sublime/" style="font-size: 15px;">sublime</a> <a href="/tags/TFRecord/" style="font-size: 15px;">TFRecord</a> <a href="/tags/FM/" style="font-size: 15px;">FM</a> <a href="/tags/指标/" style="font-size: 15px;">指标</a> <a href="/tags/auc/" style="font-size: 15px;">auc</a> <a href="/tags/zookepper/" style="font-size: 15px;">zookepper</a> <a href="/tags/centos7/" style="font-size: 15px;">centos7</a> <a href="/tags/cpm/" style="font-size: 15px;">cpm</a> <a href="/tags/opcm/" style="font-size: 15px;">opcm</a> <a href="/tags/ecpm/" style="font-size: 15px;">ecpm</a> <a href="/tags/hexo，微信屏蔽/" style="font-size: 15px;">hexo，微信屏蔽</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/tf/" style="font-size: 15px;">tf</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/重装系统/" style="font-size: 15px;">重装系统</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/OJ/" style="font-size: 15px;">OJ</a> <a href="/tags/dfs/" style="font-size: 15px;">dfs</a> <a href="/tags/递归/" style="font-size: 15px;">递归</a> <a href="/tags/poj/" style="font-size: 15px;">poj</a> <a href="/tags/回文字符串/" style="font-size: 15px;">回文字符串</a> <a href="/tags/dp/" style="font-size: 15px;">dp</a> <a href="/tags/暴力/" style="font-size: 15px;">暴力</a> <a href="/tags/棋盘规则/" style="font-size: 15px;">棋盘规则</a> <a href="/tags/状态压缩/" style="font-size: 15px;">状态压缩</a> <a href="/tags/BFS/" style="font-size: 15px;">BFS</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/pytorch/" style="font-size: 15px;">pytorch</a> <a href="/tags/module/" style="font-size: 15px;">module</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/shell/" style="font-size: 15px;">shell</a> <a href="/tags/质数/" style="font-size: 15px;">质数</a> <a href="/tags/素数/" style="font-size: 15px;">素数</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/hole/" style="font-size: 15px;">hole</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/打散/" style="font-size: 15px;">打散</a> <a href="/tags/roc-auc-score/" style="font-size: 15px;">roc_auc_score</a> <a href="/tags/svm多类别分类/" style="font-size: 15px;">svm多类别分类</a> <a href="/tags/网格搜索/" style="font-size: 15px;">网格搜索</a> <a href="/tags/textFile/" style="font-size: 15px;">textFile</a> <a href="/tags/源码剖析/" style="font-size: 15px;">源码剖析</a> <a href="/tags/rdd计算/" style="font-size: 15px;">rdd计算</a> <a href="/tags/smoothing/" style="font-size: 15px;">smoothing</a> <a href="/tags/tensorboard/" style="font-size: 15px;">tensorboard</a> <a href="/tags/test-in-ubuntu/" style="font-size: 15px;">test in ubuntu</a> <a href="/tags/tf-nn/" style="font-size: 15px;">tf.nn</a> <a href="/tags/tensor/" style="font-size: 15px;">tensor</a> <a href="/tags/gather/" style="font-size: 15px;">gather</a> <a href="/tags/cnn/" style="font-size: 15px;">cnn</a> <a href="/tags/transformer/" style="font-size: 15px;">transformer</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a> <a href="/tags/trie/" style="font-size: 15px;">trie</a> <a href="/tags/推荐，对比学习，冷启动/" style="font-size: 15px;">推荐，对比学习，冷启动</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/torch/" style="font-size: 15px;">torch</a> <a href="/tags/torch-geometric/" style="font-size: 15px;">torch-geometric</a> <a href="/tags/多任务学习/" style="font-size: 15px;">多任务学习</a> <a href="/tags/图网络/" style="font-size: 15px;">图网络</a> <a href="/tags/矩阵补全/" style="font-size: 15px;">矩阵补全</a> <a href="/tags/推荐/" style="font-size: 15px;">推荐</a> <a href="/tags/NAS/" style="font-size: 15px;">NAS</a> <a href="/tags/网络修剪/" style="font-size: 15px;">网络修剪</a> <a href="/tags/知识迁移/" style="font-size: 15px;">知识迁移</a> <a href="/tags/神经架构搜索/" style="font-size: 15px;">神经架构搜索</a> <a href="/tags/async/" style="font-size: 15px;">async</a> <a href="/tags/异步/" style="font-size: 15px;">异步</a> <a href="/tags/多进程/" style="font-size: 15px;">多进程</a> <a href="/tags/流数据/" style="font-size: 15px;">流数据</a> <a href="/tags/概念漂移/" style="font-size: 15px;">概念漂移</a> <a href="/tags/二分搜索/" style="font-size: 15px;">二分搜索</a> <a href="/tags/univesal-embedding/" style="font-size: 15px;">univesal embedding</a> <a href="/tags/机器学习工具/" style="font-size: 15px;">机器学习工具</a> <a href="/tags/universal-embedding/" style="font-size: 15px;">universal embedding</a> <a href="/tags/迁移学习/" style="font-size: 15px;">迁移学习</a> <a href="/tags/逻辑/" style="font-size: 15px;">逻辑</a> <a href="/tags/github-blog/" style="font-size: 15px;">github blog</a> <a href="/tags/例行化/" style="font-size: 15px;">例行化</a> <a href="/tags/协同过滤/" style="font-size: 15px;">协同过滤</a> <a href="/tags/GCN/" style="font-size: 15px;">GCN</a> <a href="/tags/图卷积/" style="font-size: 15px;">图卷积</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/在线学习/" style="font-size: 15px;">在线学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">九月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">八月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">七月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">五月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">四月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">十一月 2014</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://petr-mitrichev.blogspot.com/" title="petr" target="_blank">petr</a><ul></ul><a href="http://blog.pluskid.org/" title="Free Mind" target="_blank">Free Mind</a><ul></ul><a href="http://www.flickering.cn/" title="火光摇曳" target="_blank">火光摇曳</a><ul></ul><a href="https://recsys.acm.org/" title="recsys" target="_blank">recsys</a><ul></ul><a href="https://sites.google.com/site/xreborner/" title="Xreborner" target="_blank">Xreborner</a><ul></ul><a href="http://blog.watashi.ws/" title="watashi" target="_blank">watashi</a><ul></ul><a href="https://toc.csail.mit.edu/user/306" title="WJMZBMR" target="_blank">WJMZBMR</a><ul></ul><a href="http://dongxicheng.org/" title="dongxicheng" target="_blank">dongxicheng</a><ul></ul><a href="https://www.byvoid.com/zht/" title="byvoid" target="_blank">byvoid</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p><span> Copyright &copy;<a href="/." rel="nofollow">TiuVe.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><!-- 页面点击小红心，在末尾添加，避免找不到 -->
<script type="text/javascript" src="/js/love.js"></script>
<!-- 背景彩带, true打开，false关闭 --><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script><script>(function(d, s) {
  var j, e = d.getElementsByTagName('body')[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.appendChild(j);
})(document, 'script');
</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/Epsilon2.1.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>